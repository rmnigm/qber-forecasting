{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfoYSq3sfnpG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rmnigm/qber-forecasting.git\n",
        "!pip install wandb\n",
        "!pip install pytorch_lightning torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r qber-forecasting/deep_qber deep_qber"
      ],
      "metadata": {
        "id": "IWXUi_voVtlG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.functional import mean_squared_error, mean_absolute_percentage_error \n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger"
      ],
      "metadata": {
        "id": "HFfSiWyNfqPn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_qber import seed_everything, setup_dataset\n",
        "from deep_qber import TorchTSDataset, ModelInterfaceTS, ModuleTS"
      ],
      "metadata": {
        "id": "VmymWqFMTXTm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "tzXydlJCU09q",
        "outputId": "8238f366-778b-4e51-ce9f-785fa18ec982"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(123456)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV9dGAAihNe3",
        "outputId": "d86f7c63-abf6-4607-ba54-9870132203b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pulses_stats_file_path = \"/content/qber-forecasting/datasets/fr_gains.csv\"\n",
        "dataframe = pd.read_csv(pulses_stats_file_path,\n",
        "                        usecols=[0, 1, 2, 3, 4, 5, 6],\n",
        "                        engine='python'\n",
        "                        )\n",
        "dataset = dataframe.values.astype('float32')\n",
        "dataset = dataset[:100000]"
      ],
      "metadata": {
        "id": "Et_CmbTq2X2G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler = QuantileTransformer(n_quantiles=20, output_distribution=\"normal\")\n",
        "# scaler = StandardScaler()\n",
        "config = {\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"look_back\": 20,\n",
        "    \"batch_size\": 256,\n",
        "    \"epochs\": 5,\n",
        "    \"loss\": \"MSE\",\n",
        "    \"scaler\": None,\n",
        "    \"model\": \"LSTM-LeakyReLu-Dense\"\n",
        "}\n",
        "train_size = 0.8\n",
        "loss = nn.MSELoss()\n",
        "scaler = None\n",
        "\n",
        "train_loader, test_loader = setup_dataset(dataset,\n",
        "                                          config[\"look_back\"],\n",
        "                                          train_size,\n",
        "                                          config[\"scaler\"],\n",
        "                                          config[\"batch_size\"]\n",
        "                                          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62gD2uUe3SuX",
        "outputId": "f2257af8-7290-4ed1-e0b3-dc5fe57c47e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size = 80000, testing set size = 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class extract_tensor(nn.Module):\n",
        "    def forward(self,x):\n",
        "        # Output shape (batch, features, hidden)\n",
        "        tensor, _ = x\n",
        "        # Reshape shape (batch, hidden)\n",
        "        return tensor[:, -1, :]\n",
        "\n",
        "input_size = 7\n",
        "output_size = 1\n",
        "hidden_size = 128\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True),\n",
        "    extract_tensor(),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(hidden_size * 2, output_size),\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "EAh3D3HMACsN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(train_loader, test_loader, model, loss, config):\n",
        "    with wandb.init(project=\"qber-forecasting\",\n",
        "                    entity=\"rmnigm\",\n",
        "                    settings=wandb.Settings(start_method=\"thread\"),\n",
        "                    config=config,\n",
        "                    ) as run:\n",
        "        wandb_logger = WandbLogger(log_model='all')\n",
        "        checkpoint_callback = ModelCheckpoint(monitor=\"Validation MAPE\", mode=\"max\")\n",
        "\n",
        "        epochs = config[\"epochs\"]\n",
        "\n",
        "        model_interface = ModelInterfaceTS(model)\n",
        "        module = ModuleTS(model_interface, loss, lr=config[\"learning_rate\"])\n",
        "\n",
        "        trainer = pl.Trainer(logger=wandb_logger,\n",
        "                            callbacks=[checkpoint_callback],\n",
        "                            # accelerator=\"gpu\",\n",
        "                            max_epochs=epochs,\n",
        "                            )\n",
        "        \n",
        "        trainer.fit(module, train_loader, test_loader)\n",
        "\n",
        "        run.finish()"
      ],
      "metadata": {
        "id": "Qo1OO2nO_PDU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment(train_loader, test_loader, model, loss, config)"
      ],
      "metadata": {
        "id": "gdmwJ9jxBBUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}